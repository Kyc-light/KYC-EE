{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse PDF and extract content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading PDF from the Estonian Ministry of Justice and getting the content from the court case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Download PDF into donwload directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected str, bytes or os.PathLike object, not Response",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-7629a0eda60f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#download_url = \"https://www.riigiteataja.ee/kohtulahendid/fail.html?id=248280034\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#response = urllib.urlopen(download_url)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmyfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'wb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected str, bytes or os.PathLike object, not Response"
     ]
    }
   ],
   "source": [
    "url = 'https://www.riigiteataja.ee/kohtulahendid/fail.html?fid=255160096'\n",
    "myfile = requests.get(url)\n",
    "#download_url = \"https://www.riigiteataja.ee/kohtulahendid/fail.html?id=248280034\"\n",
    "#response = urllib.urlopen(download_url)\n",
    "file = open(myfile, 'wb')\n",
    "file.write(response.read())\n",
    "file.close()\n",
    "print(\"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write a for-loop to open many files -- leave a comment if you'd #like to learn how\n",
    "filename = \"C:\\\\Users\\\\patri\\\\Downloads\\\\1-19-3675 .pdf\"\n",
    "\n",
    "#open allows you to read the file\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#The while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "    \n",
    "#This if statement exists to check if the above library returned #words. It's done because PyPDF2 cannot read scanned files.\n",
    "if text != \"\":\n",
    "   text = text\n",
    "\n",
    "#If the above returns as False, we run the OCR library textract to #convert scanned/image based PDF files into text\n",
    "else:\n",
    "   text = textract.process(fileurl, method='tesseract', language='eng')\n",
    "\n",
    "# Now we have a text variable which contains all the text derived #from our PDF file. Type print(text)\n",
    "# then to see what it contains. It #likely contains a lot of spaces, possibly junk such as '\\n' etc.\n",
    "# Now, we will clean our text variable, and return it as a list of keywords."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert text into keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\patri/nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-f5315eb0f5ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#The word_tokenize() function will break our text phrases into #individual words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#we'll create a new list which contains punctuation we wish to clean\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mpunctuations\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'('\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m')'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m';'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m':'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'['\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m']'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cointegration\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \"\"\"\n\u001b[1;32m--> 144\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m     return [\n\u001b[0;32m    146\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cointegration\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    103\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m     \"\"\"\n\u001b[1;32m--> 105\u001b[1;33m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'tokenizers/punkt/{0}.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cointegration\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m     \u001b[1;31m# Load the resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m     \u001b[0mopened_resource\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raw'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cointegration\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    992\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mprotocol\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nltk'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 993\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    994\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'file'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    995\u001b[0m         \u001b[1;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cointegration\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'*'\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    700\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 701\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    703\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\patri/nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\share\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\Anaconda3\\\\envs\\\\cointegration\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\patri\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "#The word_tokenize() function will break our text phrases into #individual words\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return the synthesis of the case and if guilty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "startsynthesis = \"SÜÜDISTUS JA KOKKULEPPE SISU\"\n",
    "endsynthesis = \"KOHTU SEISUKOHT\"\n",
    "\n",
    "synthesis = text[text.find(startsynthesis):text.find(startsynthesis)]\n",
    "synthesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.find(startsynthesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n \\n \\n \\n \\nKOHTUOTSUS\\n \\nEESTI VABARIIGI NIMEL\\n \\n  \\n                                                              \\n \\n \\n \\n \\n \\nKohus\\n \\nHarju Maakohus\\n \\nTallinna kohtumaja\\n \\nOtsuse tegemise aeg ja koht\\n \\n07. mail\\n \\n2019\\n.a \\n \\nTallinn\\n \\nKriminaalasja number\\n \\n1\\n-\\n19\\n-\\n3675\\n \\n \\n(\\n19231500930\\n)\\n \\n \\n \\nKoht\\nunik\\n \\nEha Popova\\n \\n\\n \\n \\n\\n \\n \\nKriminaalasi\\n \\nRoland Sanderi\\n \\n \\n\\n \\n\\n \\n\\nkokkuleppemenetluses\\n \\n \\n\\n \\nKaidi Kahu\\n \\nS\\n\\na\\ntav\\n \\n \\n \\nRoland Sander\\n \\nik \\n37007020284, \\n \\nelukoht\\n \\nxxx\\n, \\n           \\n                              \\nE\\nesti \\nV\\nabariigi\\n \\nkodanik\\n, \\n\\nharidus,  \\n\\n,  \\nkriminaalkorras \\nkaristamata\\n \\n \\n \\n \\n \\nRESOLUTSIOON\\n \\n \\n\\n248 lg 1 p 5 ; 256\\n-\\n4,    256\\n-\\n5;  311  ja  313\\n \\n \\n\\n \\n \\nRoland \\nSander\\n \\n\\n121 lg 2 p 2 \\n \\n \\n\\n\\n1\\n \\n(\\n\\n \\nvangistust. \\n \\n \\n\\n05.05\\n.2019\\n \\n-\\n07.05\\n.2019\\n, s\\no\\n \\n3\\n \\n(k\\nolm\\n\\n11\\n \\n(\\n\\n) kuud ja 2\\n7\\n \\n\\nseitse\\n\\n \\n \\n\\nR. Sander  \\n \\nei \\npane   \\n2\\n \\n(\\nkahe\\n) aasta\\n \\n\\n2\\n \\n \\n\\n\\nR. Sanderile \\n \\nkriminaalhooldaja Tallinna Vangla \\nIda\\n-\\nHarju \\n\\nR. Sander\\n   \\n\\n\\n \\n1)\\n \\nelama alalises elukohas\\n \\nxxx\\n;\\n \\n2)\\n \\n\\nregistreerimisele;\\n \\n3)\\n \\nalluma kriminaalhooldaja kontrollile oma elukohas ning esitama talle andmeid oma \\n\\n \\n4)\\n \\nsaama kriminaalhoold\\nusametnikult eelneva loa elukohast lahkumiseks Eesti territooriumi \\npiires kauemaks kui 15\\n-\\n\\n \\n5)\\n \\nsaama kriminaalhooldusametnikult eelneva loa elu\\n-\\n\\n-\\n \\n\\n \\n6)\\n \\nsaama kriminaalhooldusametnikult eelneva loa Eesti territoori\\numilt lahkumiseks ja \\n\\n \\n \\n\\n75 lg 2 p 2\\n \\nja 8 kohustada \\n \\nR. Sanderit  \\n  \\n\\nalkoholi ja osalema sotsiaalprogrammis.\\n \\n \\nKohaldada \\nR. Sanderi \\n \\nsuhtes \\nkuni \\n\\n\\nkeeldu.\\n \\n \\nKatseaja algust arvata alates \\n07.05.2019\\n.a\\n. \\n \\n \\n\\n \\n \\n\\nR. Sanderilt\\n \\n \\nriigituludesse sun\\ndraha  \\n405\\n \\neurot ja \\nmenetluskulud \\n60\\n \\n \\neurot.  \\n \\n\\nminaalmenetluse kulud summas \\n465\\n \\neurot tuleb \\n \\n\\nmaksetena   \\n38,75\\n  \\neurot\\n.   \\nKriminaalmenetl\\n\\n\\n \\n \\nKriminaalmenetluse kulude tasumiseks vajalik makseinfo koos viitenumbri ja selgitusega \\nedastatakse kohtuistungi\\n \\n\\n\\nMakseinfo viitenumbri saab ka Harju Maakohtu Liivalaia \\n\\n-\\ntoimikust. \\n \\n\\ntasutud\\n\\n\\n \\n \\n  \\nEdasikaebamise kord\\n \\n \\n \\nKohtuotsusele ei saa esitada kokkuleppemenetlus\\n\\n\\n\\n\\n\\n\\n\\n. Apellatsioon esitatakse \\n\\n\\n3\\n \\n \\n\\n\\n\\n \\n \\n\\n \\n \\n\\nalkoholijoobe seisundis enda elukaaslasega\\n \\n\\ns\\n \\naadressil \\nx\\n\\n\\n \\n \\nM\\n \\n \\n(M\\n \\n\\n\\nmbas kannatanut juustest, \\n\\n\\n \\nlauba \\nvasakul pool haavandi ja turse, parema silmakoopa all hematoomi, vasaku silmakoopa all \\n\\n\\nvigastuse ja punetuse ning ninaverejooksu.\\n \\n \\nSel moel pani Roland Sander oma tahtliku teoga toime teise inimese tervise kahjustamise, samuti \\n\\n\\n\\n \\n \\n \\nRoland Sanderi\\n\\n\\n\\n \\np\\nikkuse\\n \\nvangistus\\nega\\n.\\n \\n\\n-\\n7.05.2019, s.o \\n\\n \\n\\n\\n\\n \\n\\n\\n \\n75 lg 1 p\\n-\\ndes 1\\n-\\n\\n\\n \\n1)\\n \\n\\n \\n2)\\n \\n\\nregistreerimisele;\\n \\n3)\\n \\nalluma kriminaalhoo\\nldaja kontrollile oma elukohas ning esitama talle andmeid oma \\n\\n \\n4)\\n \\nsaama kriminaalhooldusametnikult eelneva loa elukohast lahkumiseks Eesti territooriumi \\n\\n \\n5)\\n \\nsaama kri\\nminaalhooldusametnikult eelneva loa elu\\n-\\n\\n-\\n\\n \\n6)\\n \\nsaama kriminaalhooldusametnikult eelneva loa Eesti territooriumilt lahkumiseks ja \\n\\n \\n \\n\\n-\\nde\\nst 2\\n \\nja 8 \\n \\n\\n\\ni ajal mitte \\ntarvitama alkoholi ning \\nosalema sotsiaalprogrammis.\\n \\n \\nS\\n\\n405 eurot \\nja \\nmenetluskulud\\n, so \\nkaitsjatasu\\n \\nkohtueelses menetluses \\n60 eurot\\n. \\n \\nToimunud kokkuleppemenetluse \\n\\ntingimustes kokkuleppele. \\nRoland Sander\\n \\n\\nTallinna\\n \\nkohtumajas toimunud \\n\\n \\n\\n \\n4\\n \\n \\nKOHTU SEISUKOHT\\n \\n \\n\\n\\nelles, et \\n\\n \\nR. Sanderi\\n \\npoolt kokkuleppe \\n\\n\\n\\n\\nRoland Sander\\n \\nesitatud \\n\\n121 lg 2 p 2\\n \\n  \\n\\naristus vastavalt \\nkokkuleppele.\\n \\n \\n \\n \\n \\n \\nEha Popova\\n \\n \\nKo\\nhtunik\\n \\n \\n \\n \\n \\n \\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
